{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c741edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD Demo\").getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706eb463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('house', 1), ('good', 1), ('time', 1), ('one', 1), ('idea', 1), ('us', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Problem 1: Word Count with Filtering (Key-Value RDD)\n",
    "# Objective: Count the number of times each word appears, excluding stopwords.\n",
    "\n",
    "# Steps:\n",
    "\n",
    "# Load a list of sentences into an RDD.\n",
    "# Split each sentence into words.\n",
    "# Remove common stopwords like \"is\", \"the\", \"a\", \"an\", etc.\n",
    "# Create key-value pairs (word, 1).\n",
    "# Use reduceByKey to get the final word counts.\n",
    "\n",
    "# Transformations: flatMap, filter, map, reduceByKey\n",
    "# Actions: collect, take\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "rdd = sc.parallelize([\n",
    "                     \"It is a very good idea to be on time.\",\n",
    "                     \"She was in the house with them.\",\n",
    "                     \"You should have been there before him.\",\n",
    "                     \"They are doing it for all of us.\",\n",
    "                     \"I will not be the one to do it.\"\n",
    "                    ])\n",
    "\n",
    "\n",
    "filtered_rdd = rdd.flatMap(lambda line : [word for word in line.replace('.','').replace(',','').replace('!','').lower().split() if word not in stop_words])\n",
    "individual_word_count = filtered_rdd.map(lambda word : (word,1))\n",
    "final_count = individual_word_count.reduceByKey(lambda a,b : a + b)\n",
    "print(final_count.collect())\n",
    "\n",
    "# OR\n",
    "\n",
    "# def filter_stop_words(sentence_line):\n",
    "#     filtered = []\n",
    "#     cleaned_line = sentence_line.replace('.', '').replace(',', '').replace('!', '').lower()\n",
    "#     words = cleaned_line.split()\n",
    "#     for word in words:\n",
    "#         if word not in stop_words:\n",
    "#             filtered.append(word)\n",
    "#     return filtered\n",
    "\n",
    "# rdd = sc.parallelize([\n",
    "#                      \"It is a very good idea to be on time.\",\n",
    "#                      \"She was in the house with them.\",\n",
    "#                      \"You should have been there before him.\",\n",
    "#                      \"They are doing it for all of us.\",\n",
    "#                      \"I will not be the one to do it.\"\n",
    "#                     ])\n",
    "\n",
    "# filtered_rdd = rdd.flatMap(filter_stop_words)\n",
    "# individual_word_count = filtered_rdd.map(lambda word : (word,1))\n",
    "# final_count = individual_word_count.reduceByKey(lambda a,b : a + b)\n",
    "# print(final_count.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d248a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 75.0), ('Bob', 87.5), ('Charlie', 60.0)]\n",
      "[('Alice', 75.0), ('Bob', 87.5), ('Charlie', 60.0)]\n",
      "[('Bob', 87.5), ('Alice', 75.0), ('Charlie', 60.0)]\n"
     ]
    }
   ],
   "source": [
    "# Problem 2: Average Score per Student (Key-Value RDD)\n",
    "# Objective: Calculate average score of each student from a list of (name, score).\n",
    "\n",
    "# Input:\n",
    "# data = [(\"Alice\", 80), (\"Bob\", 90), (\"Alice\", 70), (\"Bob\", 85), (\"Charlie\", 60)]\n",
    "\n",
    "# Steps:\n",
    "\n",
    "# Map to key-value: (name, (score, 1))\n",
    "# Use reduceByKey to sum scores and counts: (name, (total_score, count))\n",
    "# Map to calculate average.\n",
    "\n",
    "# Transformations: map, reduceByKey\n",
    "# Actions: collect, takeOrdered\n",
    "\n",
    "data = [(\"Alice\", 80), (\"Bob\", 90), (\"Alice\", 70), (\"Bob\", 85), (\"Charlie\", 60)]\n",
    "rdd = sc.parallelize(data)\n",
    "key_value = rdd.map(lambda x : (x[0],(x[1],1)))\n",
    "key_value_reduce = key_value.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "averages = key_value_reduce.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "print(averages.collect())\n",
    "print(averages.takeOrdered(3))\n",
    "print(averages.takeOrdered(3,key = lambda x : -x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4139b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before swapping, result is in format (number,count)\n",
      "[(2, 1), (3, 3), (4, 2), (5, 3)]\n",
      "After swapping, result is in format (count,number)\n",
      "[(1, 2), (3, 3), (2, 4), (3, 5)]\n",
      "Sorting in descending\n",
      "[(3, 3), (3, 5), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Problem 3: Frequency of Each Number in List (List RDD)\n",
    "# Objective: From a list of numbers, count frequency of each number and sort in descending order of frequency.\n",
    "\n",
    "# Input:\n",
    "# numbers = [5, 3, 4, 5, 2, 3, 5, 3, 4]\n",
    "\n",
    "# Steps:\n",
    "\n",
    "# Convert to (number, 1)\n",
    "# Use reduceByKey to count occurrences\n",
    "# Swap to (count, number) and sort descending\n",
    "# Return top 3 frequent numbers\n",
    "\n",
    "# Transformations: map, reduceByKey, map, sortByKey\n",
    "# Actions: take, collect\n",
    "\n",
    "numbers_rdd = sc.parallelize([5, 3, 4, 5, 2, 3, 5, 3, 4])\n",
    "freq_of_num = numbers_rdd.map(lambda n : (n,1))\n",
    "final_freq = freq_of_num.reduceByKey(lambda a,b : a + b)\n",
    "print(\"Before swapping, result is in format (number,count)\")\n",
    "print(final_freq.collect())\n",
    "print(\"After swapping, result is in format (count,number)\")\n",
    "swap = final_freq.map(lambda x : (x[1],x[0]))\n",
    "print(swap.collect())\n",
    "print(\"Sorting in descending\")\n",
    "sorted_rdd = swap.sortByKey(ascending=False)\n",
    "print(sorted_rdd.take(3))\n",
    "\n",
    "# print(sorted_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bd6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
